{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- User configuration ----------\n",
        "REPO_ZIP = \"https://github.com/gitleon8301/MY-AI-Gizmo-working/archive/refs/heads/main.zip\"\n",
        "WORK_DIR = Path(\"/content/text-generation-webui\")             # where repo will live\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")      # persistent storage\n",
        "LOG_DIR = DRIVE_ROOT / \"logs\"\n",
        "HEARTBEAT_INTERVAL = 30  # seconds for silent-install heartbeat\n",
        "MODEL_TO_DOWNLOAD = os.environ.get(\"MODEL_TO_DOWNLOAD\")  # optional: model id or filename for download-model.py\n",
        "# ----------------------------------------\n",
        "\n",
        "def run_cmd(cmd, cwd=None, env=None, capture=False):\n",
        "    \"\"\"Run a shell command; return CompletedProcess.\"\"\"\n",
        "    return subprocess.run(cmd, shell=True, cwd=cwd, env=env, capture_output=capture, text=True)\n",
        "\n",
        "def stream_cmd(cmd, cwd=None, env=None, out_path=None):\n",
        "    \"\"\"\n",
        "    Stream a command's stdout/stderr live and append to out_path if provided.\n",
        "    Returns exit code.\n",
        "    \"\"\"\n",
        "    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                            cwd=cwd, env=env, text=True, bufsize=1)\n",
        "    last_output = time.time()\n",
        "    stop_flag = threading.Event()\n",
        "\n",
        "    def heartbeat():\n",
        "        while not stop_flag.wait(HEARTBEAT_INTERVAL):\n",
        "            if time.time() - last_output >= HEARTBEAT_INTERVAL:\n",
        "                print(f\"[heartbeat] still working... (no new output for ~{HEARTBEAT_INTERVAL}s)\")\n",
        "                if out_path:\n",
        "                    with open(out_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                        f.write(f\"[heartbeat] still working... (no new output for ~{HEARTBEAT_INTERVAL}s)\\n\")\n",
        "\n",
        "    hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "    hb.start()\n",
        "\n",
        "    try:\n",
        "        with open(out_path, \"a\", encoding=\"utf-8\") if out_path else None as logfile:\n",
        "            for line in proc.stdout:\n",
        "                last_output = time.time()\n",
        "                print(line, end=\"\")\n",
        "                if logfile:\n",
        "                    logfile.write(line)\n",
        "    except Exception as e:\n",
        "        print(f\"[error] stream read error: {e}\")\n",
        "    finally:\n",
        "        proc.wait()\n",
        "        stop_flag.set()\n",
        "        hb.join(timeout=1)\n",
        "    return proc.returncode\n",
        "\n",
        "def ensure_drive_dirs():\n",
        "    folders = [\n",
        "        \"models\", \"loras\", \"training\", \"characters\", \"presets\", \"prompts\",\n",
        "        \"settings\", \"chat-history\", \"instruct-history\", \"outputs\", \"images\",\n",
        "        \"logs\", \"cache\", \"extensions\", \"softprompts\"\n",
        "    ]\n",
        "    for name in folders:\n",
        "        p = DRIVE_ROOT / name\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "    LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _remove_path(p: Path):\n",
        "    try:\n",
        "        if p.is_symlink():\n",
        "            p.unlink()\n",
        "        elif p.is_dir():\n",
        "            shutil.rmtree(p)\n",
        "        elif p.exists():\n",
        "            p.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def create_link_or_copy(src: Path, dest: Path):\n",
        "    \"\"\"\n",
        "    Try to create symlink dest -> src. If symlink fails (e.g., Windows or permission),\n",
        "    fallback to copying content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "        if dest.exists() or dest.is_symlink():\n",
        "            _remove_path(dest)\n",
        "        os.symlink(str(src), str(dest), target_is_directory=src.is_dir())\n",
        "        return True\n",
        "    except Exception:\n",
        "        try:\n",
        "            if src.is_dir():\n",
        "                if dest.exists():\n",
        "                    _remove_path(dest)\n",
        "                shutil.copytree(src, dest)\n",
        "            else:\n",
        "                dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "                shutil.copy2(src, dest)\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "def cleanup_broken_models():\n",
        "    models_dir = DRIVE_ROOT / \"models\"\n",
        "    if not models_dir.exists():\n",
        "        return\n",
        "    broken = []\n",
        "    for ext in (\"*.gguf\", \"*.safetensors\", \"*.bin\", \"*.pth\", \"*.pt\"):\n",
        "        for f in models_dir.rglob(ext):\n",
        "            try:\n",
        "                if f.stat().st_size < 100 * 1024:\n",
        "                    broken.append(f)\n",
        "            except Exception:\n",
        "                pass\n",
        "    if broken:\n",
        "        for f in broken:\n",
        "            try:\n",
        "                f.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "def download_and_extract_repo():\n",
        "    \"\"\"\n",
        "    Download repo ZIP and extract to /content. Rename to WORK_DIR.\n",
        "    If WORK_DIR already exists, do nothing.\n",
        "    \"\"\"\n",
        "    if WORK_DIR.exists():\n",
        "        print(f\"[info] WORK_DIR exists: {WORK_DIR}\")\n",
        "        return True\n",
        "\n",
        "    tmp_zip = Path(\"/content/repo.zip\")\n",
        "    try:\n",
        "        tmp_zip.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    print(\"[info] downloading repository zip...\")\n",
        "    # try wget then curl\n",
        "    ok = False\n",
        "    for cmd in (\n",
        "        f\"wget -q -O {tmp_zip} {REPO_ZIP}\",\n",
        "        f\"curl -s -L -o {tmp_zip} {REPO_ZIP}\"\n",
        "    ):\n",
        "        res = run_cmd(cmd)\n",
        "        if tmp_zip.exists() and tmp_zip.stat().st_size > 1000:\n",
        "            ok = True\n",
        "            break\n",
        "    if not ok:\n",
        "        print(\"[error] download failed. Check network or REPO_ZIP.\")\n",
        "        return False\n",
        "    print(\"[info] extracting...\")\n",
        "    try:\n",
        "        run_cmd(f\"unzip -q {tmp_zip} -d /content\")\n",
        "        found = next(Path(\"/content\").glob(\"MY-AI-Gizmo-working-*\"), None)\n",
        "        if not found:\n",
        "            print(\"[error] extracted but expected folder not found\")\n",
        "            return False\n",
        "        found.rename(WORK_DIR)\n",
        "        print(f\"[info] repo extracted to {WORK_DIR}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[error] unzip/extract failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def patch_fast_install_flags():\n",
        "    \"\"\"\n",
        "    Protect against automatic llama.cpp build: rename folder if present,\n",
        "    and prepare env flags for fast install.\n",
        "    \"\"\"\n",
        "    llama_dir = WORK_DIR / \"repositories\" / \"llama.cpp\"\n",
        "    renamed = None\n",
        "    try:\n",
        "        if llama_dir.exists() and llama_dir.is_dir():\n",
        "            renamed = llama_dir.with_name(llama_dir.name + \".disabled\")\n",
        "            if renamed.exists():\n",
        "                shutil.rmtree(renamed, ignore_errors=True)\n",
        "            llama_dir.rename(renamed)\n",
        "            print(\"[info] renamed repositories/llama.cpp to prevent auto-build\")\n",
        "    except Exception:\n",
        "        print(\"[warn] unable to rename llama.cpp; installer flags will be used instead\")\n",
        "        renamed = None\n",
        "    env_flags = {\n",
        "        \"GPU_CHOICE\": \"A\",\n",
        "        \"LAUNCH_AFTER_INSTALL\": \"FALSE\",\n",
        "        \"INSTALL_EXTENSIONS\": \"FALSE\",\n",
        "        \"SKIP_LLAMACPP_BUILD\": \"TRUE\",\n",
        "        \"SKIP_TORCH_TEST\": \"TRUE\",\n",
        "        \"FORCE_CUDA\": \"FALSE\",\n",
        "        \"MPLBACKEND\": \"Agg\",\n",
        "    }\n",
        "    return renamed, env_flags\n",
        "\n",
        "def maybe_download_model(env=None):\n",
        "    \"\"\"\n",
        "    Run repo's download-model.py if MODEL_TO_DOWNLOAD env var is set.\n",
        "    This downloads models into repo's models folder; we ensure models link to Drive.\n",
        "    \"\"\"\n",
        "    if not MODEL_TO_DOWNLOAD:\n",
        "        return\n",
        "    script = WORK_DIR / \"download-model.py\"\n",
        "    if not script.exists():\n",
        "        print(\"[warn] download-model.py not found in repo; skipping model download.\")\n",
        "        return\n",
        "    print(f\"[info] downloading model: {MODEL_TO_DOWNLOAD}\")\n",
        "    cmd = f'python \"{script}\" \"{MODEL_TO_DOWNLOAD}\"'\n",
        "    rc = stream_cmd(cmd, cwd=str(WORK_DIR), env=env, out_path=str(LOG_DIR / \"download-model.log\"))\n",
        "    if rc != 0:\n",
        "        print(f\"[warn] model download exited with code {rc}\")\n",
        "\n",
        "# ---------- Main flow ----------\n",
        "\n",
        "# 1) If in Colab, try mount Drive\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import drive as gdrive\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        gdrive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] google drive mount failed: {e}\")\n",
        "\n",
        "# 2) Ensure Drive dirs & cleanup\n",
        "ensure_drive_dirs()\n",
        "cleanup_broken_models()\n",
        "\n",
        "# 3) Download or detect repo\n",
        "ok = download_and_extract_repo()\n",
        "if not ok and not WORK_DIR.exists():\n",
        "    raise SystemExit(\"[fatal] cannot obtain repository. Fix REPO_ZIP or network and retry.\")\n",
        "\n",
        "# 4) Change cwd\n",
        "os.chdir(str(WORK_DIR))\n",
        "\n",
        "# 5) Create symlinks from repo -> Drive for persistence\n",
        "links_map = [\n",
        "    (\"models\", \"models\", False),\n",
        "    (\"loras\", \"loras\", False),\n",
        "    (\"user_data/characters\", \"characters\", False),\n",
        "    (\"user_data/presets\", \"presets\", False),\n",
        "    (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "    (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "    (\"user_data/chat\", \"chat-history\", False),\n",
        "    (\"outputs\", \"outputs\", False),\n",
        "]\n",
        "for local, drive_folder, is_settings in links_map:\n",
        "    drive_path = DRIVE_ROOT / drive_folder\n",
        "    # ensure Drive path exists (settings file creation handled below)\n",
        "    if is_settings and drive_path.suffix == \"\":\n",
        "        # defensive: ensure parent exists\n",
        "        drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    else:\n",
        "        drive_path.mkdir(parents=True, exist_ok=True) if not drive_path.exists() and not drive_path.suffix else None\n",
        "    local_path = WORK_DIR / local\n",
        "    try:\n",
        "        if local_path.exists() or local_path.is_symlink():\n",
        "            _remove_path(local_path)\n",
        "    except Exception:\n",
        "        pass\n",
        "    ok = create_link_or_copy(drive_path, local_path)\n",
        "    if not ok:\n",
        "        print(f\"[warn] failed to link or copy {local} -> {drive_path}\")\n",
        "\n",
        "# 6) Ensure settings file exists on Drive; copy to repo user_data if needed\n",
        "drive_settings = DRIVE_ROOT / \"settings\" / \"settings.yaml\"\n",
        "local_settings = WORK_DIR / \"user_data\" / \"settings.yaml\"\n",
        "local_settings.parent.mkdir(parents=True, exist_ok=True)\n",
        "if drive_settings.exists() and drive_settings.stat().st_size > 0:\n",
        "    try:\n",
        "        shutil.copy2(drive_settings, local_settings)\n",
        "    except Exception:\n",
        "        pass\n",
        "else:\n",
        "    try:\n",
        "        local_settings.write_text(\"# minimal\\nlisten: true\\nshare: true\\n\")\n",
        "        # ensure Drive copy exists\n",
        "        drive_settings.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy2(local_settings, drive_settings)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# 7) Installer preparation: avoid long llama.cpp build and prepare env\n",
        "env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "renamed_llama, fast_env_flags = patch_fast_install_flags()\n",
        "\n",
        "# 8) Make start script executable if exists\n",
        "start_sh = WORK_DIR / \"start_linux.sh\"\n",
        "if start_sh.exists():\n",
        "    try:\n",
        "        start_sh.chmod(start_sh.stat().st_mode | 0o111)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# 9) Run installer if needed (live stream + log)\n",
        "installer_log = LOG_DIR / f\"installer_{int(time.time())}.log\"\n",
        "if env_marker.exists():\n",
        "    print(\"[info] virtualenv already exists; skipping full install\")\n",
        "else:\n",
        "    print(\"[info] running installer (logs -> {})\".format(installer_log))\n",
        "    env = os.environ.copy()\n",
        "    env.update(fast_env_flags)\n",
        "    # spawn installer; use stream_cmd to log and provide heartbeat\n",
        "    if start_sh.exists():\n",
        "        rc = stream_cmd(\"bash start_linux.sh\", cwd=str(WORK_DIR), env=env, out_path=str(installer_log))\n",
        "        if rc != 0:\n",
        "            print(f\"[warn] installer exited with code {rc} (see {installer_log})\")\n",
        "    else:\n",
        "        print(\"[warn] start_linux.sh not found; skipping installer step\")\n",
        "\n",
        "# 10) Optionally download a model (user sets MODEL_TO_DOWNLOAD)\n",
        "maybe_download_model(env=os.environ.copy())\n",
        "\n",
        "# 11) Attempt to restore llama.cpp name (do not auto-build - user can build later)\n",
        "if renamed_llama:\n",
        "    try:\n",
        "        dst = WORK_DIR / \"repositories\" / \"llama.cpp\"\n",
        "        if not dst.exists():\n",
        "            renamed_llama.rename(dst)\n",
        "            print(\"[info] restored repositories/llama.cpp to allow manual build later\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# 12) Final CUDA fix (best effort)\n",
        "def fix_cuda_library_path():\n",
        "    cuda_paths = [\n",
        "        '/usr/local/cuda/lib64',\n",
        "        '/usr/local/cuda-12/lib64',\n",
        "        '/usr/lib/x86_64-linux-gnu',\n",
        "        '/usr/local/nvidia/lib64',\n",
        "    ]\n",
        "    valid = []\n",
        "    for p in cuda_paths:\n",
        "        pp = Path(p)\n",
        "        if pp.exists() and any(pp.glob(\"libcuda.so*\")):\n",
        "            valid.append(p)\n",
        "    if valid:\n",
        "        os.environ['LD_LIBRARY_PATH'] = ':'.join(valid)\n",
        "\n",
        "fix_cuda_library_path()\n",
        "\n",
        "# 13) Launch server, streaming output to a log file on Drive so you keep the URL and logs\n",
        "server_log = LOG_DIR / f\"server_{int(time.time())}.log\"\n",
        "python_exe = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "cmd = f'{python_exe} -u server.py --share --listen'\n",
        "print(f\"[info] launching server with: {cmd}\")\n",
        "rc = stream_cmd(cmd, cwd=str(WORK_DIR), env=os.environ.copy(), out_path=str(server_log))\n",
        "if rc != 0:\n",
        "    print(f\"[warn] server exited with code {rc} (see {server_log})\")\n",
        "else:\n",
        "    print(f\"[info] server terminated (see {server_log})\")\n",
        "\n",
        "# End\n",
        "print(\"[done] launcher finished. Logs and persistent data are in:\", str(DRIVE_ROOT))\n"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ RECOMMENDED MODELS (COPY EXACTLY)\n",
        "üîπ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "üîπ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "üîπ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "üîπ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "üîπ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "üîπ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "‚öôÔ∏è WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}