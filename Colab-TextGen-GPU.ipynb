{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Launch AI Gizmo with Llama 2 7B (Everything Saves to Drive)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"\\033[1;32;1m\\n --> Mounting Google Drive...\\033[0;37;0m\\n\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up main directory in Google Drive\n",
        "DRIVE_PATH = '/content/drive/MyDrive/MY-AI-Gizmo'\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "os.environ.pop('PYTHONPATH', None)\n",
        "os.environ.pop('MPLBACKEND', None)\n",
        "\n",
        "# Clone or update repository\n",
        "if not Path(f'{DRIVE_PATH}/.git').exists():\n",
        "  print(\"\\033[1;32;1m\\n --> Installing AI Gizmo in Google Drive (first time setup)...\\033[0;37;0m\\n\")\n",
        "\n",
        "  %cd /content/drive/MyDrive\n",
        "\n",
        "  # Clone the repo\n",
        "  !git clone https://github.com/gitleon8301/MY-AI-Gizmo-working.git MY-AI-Gizmo\n",
        "  %cd MY-AI-Gizmo\n",
        "\n",
        "  # Install dependencies\n",
        "  print(\"\\033[1;32;1m --> Installing dependencies...\\033[0;37;0m\\n\")\n",
        "  !pip install -q -r requirements.txt\n",
        "\n",
        "else:\n",
        "  print(\"\\033[1;32;1m\\n --> Using existing AI Gizmo installation from Drive\\033[0;37;0m\\n\")\n",
        "  %cd {DRIVE_PATH}\n",
        "\n",
        "  # Update the repo\n",
        "  !git pull\n",
        "\n",
        "# Create necessary directories in Drive\n",
        "for folder in ['models', 'characters', 'presets', 'prompts', 'user_data', 'logs', 'extensions']:\n",
        "    os.makedirs(f'{DRIVE_PATH}/{folder}', exist_ok=True)\n",
        "\n",
        "# Detect GPU\n",
        "try:\n",
        "    import torch\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    if has_gpu:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"\\033[1;32;1m ‚úì GPU detected: {gpu_name}\\033[0;37;0m\")\n",
        "    else:\n",
        "        print(\"\\033[1;33;1m ‚ö† No GPU detected - using CPU mode\\033[0;37;0m\")\n",
        "except:\n",
        "    has_gpu = False\n",
        "    print(\"\\033[1;33;1m ‚ö† No GPU detected - using CPU mode\\033[0;37;0m\")\n",
        "\n",
        "# Model configuration - Llama 2 7B Chat\n",
        "model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\"\n",
        "model_file = \"llama-2-7b-chat.Q4_K_M.gguf\"  # 4-bit quantized, good balance\n",
        "output_folder = \"TheBloke_Llama-2-7B-Chat-GGUF\"\n",
        "\n",
        "model_path = Path(f\"{DRIVE_PATH}/models/{output_folder}\")\n",
        "\n",
        "# Download model if not exists\n",
        "if not model_path.exists():\n",
        "    print(f\"\\033[1;32;1m --> Downloading Llama 2 7B Chat to Drive...\\033[0;37;0m\\n\")\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "    # Download using huggingface-cli\n",
        "    !pip install -q huggingface_hub\n",
        "\n",
        "    from huggingface_hub import hf_hub_download\n",
        "\n",
        "    try:\n",
        "        downloaded_file = hf_hub_download(\n",
        "            repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
        "            filename=model_file,\n",
        "            local_dir=str(model_path),\n",
        "            local_dir_use_symlinks=False\n",
        "        )\n",
        "        print(f\"\\033[1;32;1m ‚úì Model downloaded successfully!\\033[0;37;0m\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\033[1;31;1m ‚úó Download failed: {e}\\033[0;37;0m\\n\")\n",
        "else:\n",
        "    # Check if model file exists\n",
        "    if (model_path / model_file).exists():\n",
        "        print(f\"\\033[1;32;1m ‚úì Llama 2 7B already exists in Drive\\033[0;37;0m\\n\")\n",
        "    else:\n",
        "        print(f\"\\033[1;33;1m ‚ö† Model folder exists but file missing. Re-downloading...\\033[0;37;0m\\n\")\n",
        "\n",
        "        !pip install -q huggingface_hub\n",
        "        from huggingface_hub import hf_hub_download\n",
        "\n",
        "        try:\n",
        "            downloaded_file = hf_hub_download(\n",
        "                repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
        "                filename=model_file,\n",
        "                local_dir=str(model_path),\n",
        "                local_dir_use_symlinks=False\n",
        "            )\n",
        "            print(f\"\\033[1;32;1m ‚úì Model downloaded successfully!\\033[0;37;0m\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\033[1;31;1m ‚úó Download failed: {e}\\033[0;37;0m\\n\")\n",
        "\n",
        "# Configure launch command\n",
        "if has_gpu:\n",
        "    # GPU settings for T4\n",
        "    cmd = f\"\"\"python server.py \\\n",
        "    --model {output_folder}/{model_file} \\\n",
        "    --n-gpu-layers 35 \\\n",
        "    --api \\\n",
        "    --share \\\n",
        "    --listen \\\n",
        "    --verbose\"\"\"\n",
        "else:\n",
        "    # CPU settings\n",
        "    cmd = f\"\"\"python server.py \\\n",
        "    --model {output_folder}/{model_file} \\\n",
        "    --cpu \\\n",
        "    --threads 2 \\\n",
        "    --api \\\n",
        "    --share \\\n",
        "    --listen \\\n",
        "    --verbose\"\"\"\n",
        "\n",
        "# Display storage info\n",
        "print(\"\\n\" + \"\\033[1;36;1m\" + \"=\"*80)\n",
        "print(\"üíæ PERSISTENT STORAGE:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"üìÅ Root:        {DRIVE_PATH}\")\n",
        "print(f\"ü§ñ Model:       {DRIVE_PATH}/models/{output_folder}\")\n",
        "print(f\"üí¨ Chats:       {DRIVE_PATH}/characters\")\n",
        "print(f\"üìù All data saves automatically to Google Drive!\")\n",
        "print(\"=\"*80 + \"\\033[0;37;0m\\n\")\n",
        "\n",
        "# Start the server\n",
        "print(\"\\n\" + \"\\033[1;35;1m\" + \"=\"*80)\n",
        "print(\"üöÄ STARTING LLAMA 2 7B CHAT\")\n",
        "print(\"=\"*80 + \"\\033[0;37;0m\\n\")\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    cmd,\n",
        "    shell=True,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    universal_newlines=True,\n",
        "    bufsize=1,\n",
        "    cwd=DRIVE_PATH\n",
        ")\n",
        "\n",
        "# Monitor for URLs\n",
        "local_url = None\n",
        "public_url = None\n",
        "urls_displayed = False\n",
        "\n",
        "for line in iter(process.stdout.readline, ''):\n",
        "    print(line, end='')\n",
        "\n",
        "    # Capture URLs\n",
        "    if not local_url:\n",
        "        local_match = re.search(r'(http://(?:127\\.0\\.0\\.1|0\\.0\\.0\\.0|localhost):\\d+)', line)\n",
        "        if local_match:\n",
        "            local_url = local_match.group(1)\n",
        "\n",
        "    if not public_url:\n",
        "        public_match = re.search(r'(https://[a-z0-9\\-]+\\.gradio\\.live)', line)\n",
        "        if public_match:\n",
        "            public_url = public_match.group(1)\n",
        "\n",
        "    # Display when ready\n",
        "    if local_url and public_url and not urls_displayed:\n",
        "        print(\"\\n\" + \"\\033[1;32;1m\" + \"=\"*80)\n",
        "        print(\"üéâ LLAMA 2 7B CHAT IS READY!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nüìç LOCAL URL:  {local_url}\")\n",
        "        print(f\"üåê PUBLIC URL: {public_url}\")\n",
        "        print(f\"\\nüí° Click the PUBLIC URL to start chatting!\")\n",
        "        print(f\"üíæ All conversations save to: {DRIVE_PATH}/characters\")\n",
        "        print(\"=\"*80 + \"\\033[0;37;0m\\n\")\n",
        "        urls_displayed = True\n",
        "\n",
        "process.wait()"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}