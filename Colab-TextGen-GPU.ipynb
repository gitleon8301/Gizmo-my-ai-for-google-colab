{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# MY-AI-GIZMO FINAL WORKING VERSION\n",
        "# Fixes indentation + Auto-detects and displays URL\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "import time\n",
        "import re\n",
        "import threading\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ğŸš€ MY-AI-GIZMO FINAL INSTALLER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# HELPERS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def run_cmd(cmd, check=False, quiet=True, timeout=None):\n",
        "    try:\n",
        "        if quiet:\n",
        "            return subprocess.run(cmd, check=check, capture_output=True, text=True, timeout=timeout)\n",
        "        else:\n",
        "            return subprocess.run(cmd, check=check, timeout=timeout)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def safe_pip(packages):\n",
        "    if isinstance(packages, str):\n",
        "        packages = [packages]\n",
        "    for pkg in packages:\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "                         check=False, timeout=300, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "def extract_url(text):\n",
        "    match = re.search(r'https://[a-z0-9]+\\.gradio\\.live', text, re.IGNORECASE)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "def print_url_box(url, hw):\n",
        "    box = \"=\"*70\n",
        "    print(\"\\n\" + box)\n",
        "    print(\"ğŸ‰\"*35)\n",
        "    print(box)\n",
        "    print(\"âœ… SERVER IS RUNNING!\")\n",
        "    print(box)\n",
        "    print(f\"\\nğŸŒ PUBLIC URL (CLICK TO OPEN):\\n\")\n",
        "    print(f\"   {url}\\n\")\n",
        "    print(box)\n",
        "    print(f\"ğŸ’¾ All chats save to Google Drive\")\n",
        "    print(f\"ğŸ–¥ï¸  Hardware: {hw}\")\n",
        "    print(f\"â¹ï¸  Press Ctrl+C to stop\")\n",
        "    print(box + \"\\n\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PATHS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "DRIVE_BASE = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "REPO_DIR = DRIVE_BASE / \"MY-AI-Gizmo-working\"\n",
        "MODELS_DIR = DRIVE_BASE / \"models\"\n",
        "USER_DATA_DIR = DRIVE_BASE / \"user_data\"\n",
        "MODEL_FILE = MODELS_DIR / \"llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 1: MOUNT DRIVE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ“‚ Step 1: Google Drive...\")\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    mydrive = Path(\"/content/drive/MyDrive\")\n",
        "\n",
        "    if mydrive.exists() and mydrive.is_dir():\n",
        "        print(\"   âœ… Mounted\")\n",
        "    else:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"   âœ… Mounted\")\n",
        "\n",
        "    if not mydrive.exists():\n",
        "        raise Exception(\"Mount failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   âŒ Error: {e}\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "for d in [DRIVE_BASE, MODELS_DIR, USER_DATA_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 2-3: SYSTEM + REPO\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ“¦ Step 2: System...\")\n",
        "run_cmd([\"apt-get\", \"update\", \"-qq\"])\n",
        "run_cmd([\"apt-get\", \"install\", \"-y\", \"-qq\", \"build-essential\", \"cmake\", \"git\", \"wget\"])\n",
        "print(\"   âœ… Done\")\n",
        "\n",
        "print(\"\\nğŸ“¥ Step 3: Repository...\")\n",
        "\n",
        "if (REPO_DIR / \"server.py\").exists():\n",
        "    print(\"   âœ… Exists\")\n",
        "    os.chdir(REPO_DIR)\n",
        "else:\n",
        "    REPO_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "    os.chdir(REPO_DIR.parent)\n",
        "    run_cmd([\"git\", \"clone\", \"https://github.com/gitleon8301/MY-AI-Gizmo-working.git\"])\n",
        "    print(\"   âœ… Cloned\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 4: PACKAGES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ Step 4: Packages...\")\n",
        "\n",
        "safe_pip([\"setuptools\", \"wheel\", \"numpy\", \"requests\", \"tqdm\", \"pyyaml\"])\n",
        "\n",
        "env = dict(os.environ)\n",
        "env[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=off -DLLAMA_BUILD_SERVER=ON\"\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"llama-cpp-python[server]\", \"--no-cache-dir\", \"-q\"],\n",
        "              check=False, env=env, timeout=600, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "safe_pip([\"torch\", \"transformers\", \"gradio>=3.50.0\", \"accelerate\", \"markdown\",\n",
        "         \"Pillow\", \"safetensors\", \"sentencepiece\", \"protobuf\"])\n",
        "\n",
        "print(\"   âœ… Done\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 5: FIX UI - PROPER INDENTATION FIX\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ”§ Step 5: Fixing UI (indentation)...\")\n",
        "\n",
        "ui_file = REPO_DIR / \"modules\" / \"ui.py\"\n",
        "\n",
        "if ui_file.exists():\n",
        "    with open(ui_file, 'r', encoding='utf-8') as f:\n",
        "        original_content = f.read()\n",
        "\n",
        "    if 'button_shadow_hover' in original_content and '# COLAB_FIXED' not in original_content:\n",
        "        lines = original_content.split('\\n')\n",
        "        fixed_lines = []\n",
        "        skip_mode = False\n",
        "        skip_depth = 0\n",
        "        base_indent = 0\n",
        "\n",
        "        i = 0\n",
        "        while i < len(lines):\n",
        "            line = lines[i]\n",
        "\n",
        "            # Find the problematic section\n",
        "            if 'if not shared.args.old_colors:' in line and not skip_mode:\n",
        "                # Get base indentation\n",
        "                base_indent = len(line) - len(line.lstrip())\n",
        "                indent_str = ' ' * base_indent\n",
        "\n",
        "                # Add commented-out replacement\n",
        "                fixed_lines.append(indent_str + '# COLAB_FIXED - Theme disabled for compatibility')\n",
        "                fixed_lines.append(indent_str + 'if False:')\n",
        "                fixed_lines.append(indent_str + '    pass')\n",
        "\n",
        "                # Enter skip mode - we'll skip until we find the matching closing\n",
        "                skip_mode = True\n",
        "                skip_depth = 0\n",
        "                i += 1\n",
        "\n",
        "                # Skip the next line if it's the theme.set( start\n",
        "                while i < len(lines):\n",
        "                    current = lines[i]\n",
        "\n",
        "                    # Count parentheses\n",
        "                    skip_depth += current.count('(') - current.count(')')\n",
        "\n",
        "                    # If we've closed all parens, we're done\n",
        "                    if skip_depth <= 0 and ')' in current:\n",
        "                        skip_mode = False\n",
        "                        i += 1\n",
        "                        break\n",
        "\n",
        "                    i += 1\n",
        "\n",
        "                continue\n",
        "\n",
        "            if not skip_mode:\n",
        "                fixed_lines.append(line)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        # Write fixed version\n",
        "        with open(ui_file, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(fixed_lines))\n",
        "\n",
        "        print(\"   âœ… Fixed indentation\")\n",
        "    else:\n",
        "        print(\"   âœ… Already OK\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 6: FIX LLAMA SERVER\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ”§ Step 6: Llama server...\")\n",
        "\n",
        "LLAMA_CODE = '''\"\"\"llama.cpp server - no llama_cpp_binaries\"\"\"\n",
        "import json, torch, os, socket, subprocess, sys, threading, time, requests\n",
        "from pathlib import Path\n",
        "from modules import shared\n",
        "from modules.logging_colors import logger\n",
        "\n",
        "def get_llama_server_path():\n",
        "    try:\n",
        "        import llama_cpp\n",
        "        for p in [Path(llama_cpp.__file__).parent/\"server\"/\"llama-server\", Path(sys.prefix)/\"bin\"/\"llama-server\"]:\n",
        "            if p.exists(): return str(p)\n",
        "        r = subprocess.run(['which','llama-server'], capture_output=True, text=True, timeout=5)\n",
        "        if r.returncode == 0: return r.stdout.strip()\n",
        "    except: pass\n",
        "    return \"llama-server\"\n",
        "\n",
        "class LlamaServer:\n",
        "    def __init__(self, model_path, server_path=None):\n",
        "        self.model_path = model_path\n",
        "        self.server_path = server_path or get_llama_server_path()\n",
        "        self.port = self._find_port()\n",
        "        self.process = None\n",
        "        self.session = requests.Session()\n",
        "        self.vocabulary_size = None\n",
        "        self.bos_token = \"~~\"\n",
        "        self.last_prompt_token_count = 0\n",
        "        self._start_server()\n",
        "\n",
        "    def encode(self, text, add_bos_token=False, **kwargs):\n",
        "        if self.bos_token and text.startswith(self.bos_token): add_bos_token = False\n",
        "        r = self.session.post(f\"http://127.0.0.1:{self.port}/tokenize\", json={\"content\":text,\"add_special\":add_bos_token})\n",
        "        return r.json().get(\"tokens\",[])\n",
        "\n",
        "    def decode(self, token_ids, **kwargs):\n",
        "        r = self.session.post(f\"http://127.0.0.1:{self.port}/detokenize\", json={\"tokens\":token_ids})\n",
        "        return r.json().get(\"content\",\"\")\n",
        "\n",
        "    def prepare_payload(self, state):\n",
        "        temp = state[\"temperature\"]\n",
        "        if state.get(\"dynamic_temperature\"): temp = (state[\"dynatemp_low\"]+state[\"dynatemp_high\"])/2\n",
        "        return {\"temperature\":temp,\"top_k\":state.get(\"top_k\",40),\"top_p\":state.get(\"top_p\",0.95),\n",
        "                \"min_p\":state.get(\"min_p\",0.05),\"repeat_penalty\":state.get(\"repetition_penalty\",1.1),\"seed\":state.get(\"seed\",-1)}\n",
        "\n",
        "    def is_multimodal(self): return False\n",
        "\n",
        "    def generate_with_streaming(self, prompt, state):\n",
        "        payload = self.prepare_payload(state)\n",
        "        tokens = self.encode(prompt, add_bos_token=state.get(\"add_bos_token\",False))\n",
        "        self.last_prompt_token_count = len(tokens)\n",
        "        payload[\"prompt\"] = tokens\n",
        "        payload.update({\"n_predict\":state.get('max_new_tokens',200),\"stream\":True,\"cache_prompt\":True})\n",
        "        r = self.session.post(f\"http://127.0.0.1:{self.port}/completion\", json=payload, stream=True)\n",
        "        full = \"\"\n",
        "        try:\n",
        "            for line in r.iter_lines():\n",
        "                if not line: continue\n",
        "                try:\n",
        "                    line = line.decode('utf-8')\n",
        "                    if line.startswith('data: '): line = line[6:]\n",
        "                    data = json.loads(line)\n",
        "                    if data.get('content'): full += data['content']; yield full\n",
        "                    if data.get('stop'): break\n",
        "                except: continue\n",
        "        finally: r.close()\n",
        "\n",
        "    def generate(self, prompt, state):\n",
        "        out = \"\"\n",
        "        for out in self.generate_with_streaming(prompt, state): pass\n",
        "        return out\n",
        "\n",
        "    def _get_vocabulary_size(self):\n",
        "        try:\n",
        "            r = self.session.get(f\"http://127.0.0.1:{self.port}/v1/models\").json()\n",
        "            if r.get(\"data\") and r[\"data\"]:\n",
        "                meta = r[\"data\"][0].get(\"meta\",{})\n",
        "                if \"n_vocab\" in meta: self.vocabulary_size = meta[\"n_vocab\"]\n",
        "        except: pass\n",
        "\n",
        "    def _get_bos_token(self):\n",
        "        try:\n",
        "            r = self.session.get(f\"http://127.0.0.1:{self.port}/props\").json()\n",
        "            if \"bos_token\" in r: self.bos_token = r[\"bos_token\"]\n",
        "        except: pass\n",
        "\n",
        "    def _find_port(self):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.bind(('',0))\n",
        "            return s.getsockname()[1]\n",
        "\n",
        "    def _start_server(self):\n",
        "        has_gpu = torch.cuda.is_available()\n",
        "        gpu_layers = getattr(shared.args,'gpu_layers',0) if has_gpu else 0\n",
        "        if has_gpu: logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        else: logger.info(\"CPU mode\")\n",
        "        cmd = [self.server_path,\"--model\",self.model_path,\"--ctx-size\",str(getattr(shared.args,'ctx_size',2048)),\n",
        "               \"--batch-size\",str(getattr(shared.args,'batch_size',512)),\"--port\",str(self.port),\"--no-webui\"]\n",
        "        if has_gpu and gpu_layers > 0: cmd += [\"--gpu-layers\",str(gpu_layers)]\n",
        "        threads = getattr(shared.args,'threads',0)\n",
        "        if threads > 0: cmd += [\"--threads\",str(threads)]\n",
        "        logger.info(f\"Starting llama.cpp on port {self.port}\")\n",
        "        self.process = subprocess.Popen(cmd, stderr=subprocess.PIPE, bufsize=0)\n",
        "        threading.Thread(target=self._log_stderr, daemon=True).start()\n",
        "        health = f\"http://127.0.0.1:{self.port}/health\"\n",
        "        for _ in range(60):\n",
        "            if self.process.poll() is not None: raise RuntimeError(f\"Server died: {self.process.returncode}\")\n",
        "            try:\n",
        "                if self.session.get(health, timeout=1).status_code == 200: break\n",
        "            except: pass\n",
        "            time.sleep(1)\n",
        "        self._get_vocabulary_size()\n",
        "        self._get_bos_token()\n",
        "        logger.info(\"Ready\")\n",
        "\n",
        "    def _log_stderr(self):\n",
        "        try:\n",
        "            for line in iter(self.process.stderr.readline, b''):\n",
        "                print(line.decode('utf-8',errors='replace').strip(), file=sys.stderr)\n",
        "        except: pass\n",
        "\n",
        "    def stop(self):\n",
        "        if self.process:\n",
        "            self.process.terminate()\n",
        "            try: self.process.wait(timeout=5)\n",
        "            except: self.process.kill()\n",
        "\n",
        "    def __del__(self): self.stop()\n",
        "'''\n",
        "\n",
        "(REPO_DIR / \"modules\" / \"llama_cpp_server.py\").write_text(LLAMA_CODE, encoding='utf-8')\n",
        "print(\"   âœ… Fixed\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEPS 7-9: USER DATA, MODEL, LINKS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ”— Step 7: User data...\")\n",
        "local_data = REPO_DIR / \"user_data\"\n",
        "if local_data.exists() and not local_data.is_symlink():\n",
        "    for item in local_data.rglob(\"*\"):\n",
        "        if item.is_file():\n",
        "            rel = item.relative_to(local_data)\n",
        "            dest = USER_DATA_DIR / rel\n",
        "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(item, dest)\n",
        "    shutil.rmtree(local_data)\n",
        "if not local_data.exists():\n",
        "    local_data.symlink_to(USER_DATA_DIR)\n",
        "for sub in [\"logs\",\"logs/chat\",\"logs/instruct\",\"presets\",\"characters\"]:\n",
        "    (USER_DATA_DIR / sub).mkdir(parents=True, exist_ok=True)\n",
        "print(\"   âœ… Linked\")\n",
        "\n",
        "print(\"\\nâ¬‡ï¸  Step 8: Model...\")\n",
        "if MODEL_FILE.exists():\n",
        "    print(f\"   âœ… Exists ({MODEL_FILE.stat().st_size/(1024**3):.2f} GB)\")\n",
        "else:\n",
        "    print(\"   ğŸ“¥ Downloading...\")\n",
        "    MODEL_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "    run_cmd([\"wget\",\"-q\",\"--show-progress\",\"-O\",str(MODEL_FILE),\n",
        "            \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\"],\n",
        "            quiet=False, timeout=900)\n",
        "    print(f\"   âœ… Done\")\n",
        "\n",
        "print(\"\\nğŸ”— Step 9: Models...\")\n",
        "repo_models = REPO_DIR / \"models\"\n",
        "if not repo_models.is_symlink():\n",
        "    if repo_models.exists():\n",
        "        shutil.rmtree(repo_models)\n",
        "    repo_models.symlink_to(MODELS_DIR)\n",
        "print(\"   âœ… Linked\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 10: HARDWARE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ–¥ï¸  Step 10: Hardware...\")\n",
        "has_gpu = False\n",
        "try:\n",
        "    import torch\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    print(f\"   {'âœ… GPU: '+torch.cuda.get_device_name(0) if has_gpu else 'â„¹ï¸  CPU'}\")\n",
        "except:\n",
        "    print(\"   â„¹ï¸  CPU\")\n",
        "\n",
        "hw = \"GPU\" if has_gpu else \"CPU\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 11: LAUNCH WITH URL MONITORING\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nâš™ï¸  Step 11: Launching...\")\n",
        "\n",
        "cmd = [sys.executable, \"server.py\"]\n",
        "if has_gpu:\n",
        "    cmd.extend([\"--gpu-layers\", \"35\"])\n",
        "else:\n",
        "    cmd.extend([\"--cpu\"])\n",
        "cmd.extend([\"--threads\",\"4\",\"--listen\",\"--listen-host\",\"0.0.0.0\",\"--share\",\n",
        "            \"--model\",str(MODEL_FILE),\"--loader\",\"llama.cpp\"])\n",
        "\n",
        "print(f\"   Hardware: {hw}\")\n",
        "print(f\"   Model: {MODEL_FILE.name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸš€ STARTING SERVER\")\n",
        "print(\"=\"*70)\n",
        "print(\"â³ Please wait 30-60 seconds...\")\n",
        "print(\"ğŸ” Monitoring for public URL...\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# Global URL storage\n",
        "found_url = [None]  # Use list to modify in thread\n",
        "url_printed = [False]\n",
        "\n",
        "def url_reminder():\n",
        "    \"\"\"Print URL reminder every 2 minutes if not found\"\"\"\n",
        "    while True:\n",
        "        time.sleep(120)  # 2 minutes\n",
        "        if not url_printed[0]:\n",
        "            print(\"\\n\" + \"â³\"*35)\n",
        "            print(\"Still waiting for server to start...\")\n",
        "            print(\"The Gradio URL will appear above when ready\")\n",
        "            print(\"â³\"*35 + \"\\n\")\n",
        "\n",
        "# Start reminder thread\n",
        "reminder_thread = threading.Thread(target=url_reminder, daemon=True)\n",
        "reminder_thread.start()\n",
        "\n",
        "try:\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                              universal_newlines=True, bufsize=1)\n",
        "\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "\n",
        "        # Check for URL\n",
        "        if not url_printed[0]:\n",
        "            url = extract_url(line)\n",
        "            if url:\n",
        "                found_url[0] = url\n",
        "                url_printed[0] = True\n",
        "                print_url_box(url, hw)\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"â¹ï¸  Server stopped\")\n",
        "    if found_url[0]:\n",
        "        print(f\"ğŸ“ URL was: {found_url[0]}\")\n",
        "    print(f\"ğŸ’¾ Data: {USER_DATA_DIR}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Error: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Done\")"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}