{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonlazdev-wq/Gizmo-my-ai-for-google-colab/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ================================================================\n",
        "# MY-AI-Gizmo ‚Ä¢ UNIVERSAL LAUNCHER  v3.5.3 - COLAB READY\n",
        "# ================================================================\n",
        "# FIXES:\n",
        "#  ‚úÖ Loads ALL required extensions (lesson + utils + model hub)\n",
        "#  ‚úÖ Gradio Timer shim for 4.37.x (prevents startup crash)\n",
        "#  ‚úÖ Cleaner URL capture + restart loop\n",
        "# ================================================================\n",
        "\n",
        "import os, sys, subprocess, shutil, re, time, threading\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "try:\n",
        "    from google.colab import drive as colab_drive\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    colab_drive = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "# ---------------- Repo ----------------\n",
        "GITHUB_USER   = \"leonlazdev-wq\"\n",
        "GITHUB_REPO   = \"Gizmo-my-ai-for-google-colab\"\n",
        "GITHUB_BRANCH = \"main\"\n",
        "REPO_ZIP       = \"\"\n",
        "REPO_CLONE_URL = \"\"\n",
        "\n",
        "# ---------------- Paths ----------------\n",
        "WORK_DIR           = Path(\"/content/text-generation-webui\")\n",
        "DRIVE_ROOT         = None\n",
        "LOG_DIR            = None\n",
        "MPL_CONFIG_DIR     = None\n",
        "PUBLIC_URL_FILE    = None\n",
        "HEARTBEAT_INTERVAL = 30\n",
        "MAX_RESTARTS       = 3\n",
        "\n",
        "# ---------------- Model menu ----------------\n",
        "MODEL_MENU = [\n",
        "    (\"1  TinyLlama-1.1B  Q4_K_M  [~0.7 GB]\", \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\", \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\", 0.7),\n",
        "    (\"2  Phi-3-mini-4k   Q4_K_M  [~2.2 GB]\", \"bartowski/Phi-3-mini-4k-instruct-GGUF\", \"Phi-3-mini-4k-instruct-Q4_K_M.gguf\", 2.2),\n",
        "    (\"3  Mistral-7B-v0.3 Q4_K_M  [~4.4 GB]\", \"bartowski/Mistral-7B-v0.3-GGUF\", \"Mistral-7B-v0.3-Q4_K_M.gguf\", 4.4),\n",
        "    (\"4  Qwen2.5-Coder-7B Q4_K_M [~4.7 GB]\", \"Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\", \"qwen2.5-coder-7b-instruct-q4_k_m.gguf\", 4.7),\n",
        "    (\"5  Qwen2.5-Coder-14B Q4_K_M [~8.9 GB]\", \"Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\", \"qwen2.5-coder-14b-instruct-q4_k_m.gguf\", 8.9),\n",
        "    (\"6  Custom (enter HF repo + filename)\", \"\", \"\", 0),\n",
        "]\n",
        "\n",
        "# ---------------- Globals ----------------\n",
        "GITHUB_TOKEN = \"\"\n",
        "MODEL_REPO   = \"\"\n",
        "MODEL_FILE   = \"\"\n",
        "USE_MODEL    = False\n",
        "GPU_LAYERS   = -1\n",
        "N_CTX        = 4096\n",
        "USE_GPU      = True\n",
        "\n",
        "# IMPORTANT: full extension set\n",
        "EXTENSIONS = \"gizmo_toolbar,dual_model,google_workspace,learning_center,student_utils,model_hub\"\n",
        "\n",
        "URL_PATTERNS = [\n",
        "    re.compile(r'Running on public URL:\\s*(https?://\\S+)', re.IGNORECASE),\n",
        "    re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.gradio\\.live\\S*)', re.IGNORECASE),\n",
        "    re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com\\S*)', re.IGNORECASE),\n",
        "    re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.ngrok\\S*)', re.IGNORECASE),\n",
        "]\n",
        "URL_KEYWORDS = (\"gradio.live\", \"trycloudflare.com\", \"ngrok\", \"loca.lt\")\n",
        "\n",
        "\n",
        "# ---------------- Utility ----------------\n",
        "def sh(cmd, cwd=None, env=None):\n",
        "    return subprocess.run(cmd, shell=True, cwd=cwd, env=env, capture_output=True, text=True)\n",
        "\n",
        "def get_free_ram_gb():\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemAvailable\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def get_total_ram_gb():\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemTotal\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def auto_thread_count():\n",
        "    try:\n",
        "        import multiprocessing\n",
        "        return max(1, min(multiprocessing.cpu_count() - 1, 4))\n",
        "    except Exception:\n",
        "        return 2\n",
        "\n",
        "def auto_ctx_size(model_gb):\n",
        "    free = get_free_ram_gb() - model_gb - 0.5\n",
        "    if free >= 2.0: return 4096\n",
        "    if free >= 1.0: return 2048\n",
        "    if free >= 0.5: return 1024\n",
        "    return 512\n",
        "\n",
        "def print_ram_status():\n",
        "    free = get_free_ram_gb()\n",
        "    total = get_total_ram_gb()\n",
        "    used = total - free\n",
        "    pct = (used / total) if total else 0\n",
        "    bar = \"‚ñà\" * int(pct * 20) + \"‚ñë\" * (20 - int(pct * 20))\n",
        "    print(f\"RAM [{bar}] {used:.1f}/{total:.1f} GB ({free:.1f} GB free)\")\n",
        "\n",
        "def _kill_old_servers():\n",
        "    sh(\"pkill -9 -f 'python.*server.py'\")\n",
        "    sh(\"pkill -9 -f 'python.*gradio'\")\n",
        "    sh(\"pkill -9 -f '_gizmo_launch'\")\n",
        "    time.sleep(2)\n",
        "\n",
        "\n",
        "# ---------------- Token ----------------\n",
        "def _token_file_path():\n",
        "    if Path(\"/content/drive/MyDrive\").exists():\n",
        "        return Path(\"/content/drive/MyDrive/MY-AI-Gizmo/github_token.txt\")\n",
        "    return Path(\"/content/MY-AI-Gizmo/github_token.txt\")\n",
        "\n",
        "def _load_saved_token():\n",
        "    for candidate in (\n",
        "        Path(\"/content/drive/MyDrive/MY-AI-Gizmo/github_token.txt\"),\n",
        "        Path(\"/content/MY-AI-Gizmo/github_token.txt\"),\n",
        "    ):\n",
        "        if candidate.exists():\n",
        "            try:\n",
        "                tok = candidate.read_text(encoding=\"utf-8\").strip()\n",
        "                if len(tok) >= 10:\n",
        "                    return tok\n",
        "            except Exception:\n",
        "                pass\n",
        "    return \"\"\n",
        "\n",
        "def _save_token(token):\n",
        "    p = _token_file_path()\n",
        "    try:\n",
        "        p.parent.mkdir(parents=True, exist_ok=True)\n",
        "        p.write_text(token, encoding=\"utf-8\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Could not save token: {e}\")\n",
        "\n",
        "def _build_urls():\n",
        "    global REPO_ZIP, REPO_CLONE_URL\n",
        "    REPO_ZIP = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}/archive/refs/heads/{GITHUB_BRANCH}.zip\"\n",
        "    REPO_CLONE_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}.git\"\n",
        "\n",
        "def setup_github_token():\n",
        "    global GITHUB_TOKEN\n",
        "    print(\"=\" * 70)\n",
        "    print(\"MY-AI-Gizmo v3.5.3 ‚Äî GitHub Authentication\")\n",
        "    print(\"=\" * 70)\n",
        "    saved = _load_saved_token()\n",
        "    if saved:\n",
        "        ans = input(f\"Use saved token (...{saved[-3:]})? (y/n): \").strip().lower()\n",
        "        if ans != \"n\":\n",
        "            GITHUB_TOKEN = saved\n",
        "            _build_urls()\n",
        "            return\n",
        "\n",
        "    while True:\n",
        "        token = input(\"Paste GitHub token: \").strip()\n",
        "        if token:\n",
        "            GITHUB_TOKEN = token\n",
        "            break\n",
        "        print(\"Token cannot be empty.\")\n",
        "    _build_urls()\n",
        "    _save_token(GITHUB_TOKEN)\n",
        "\n",
        "\n",
        "# ---------------- Drive ----------------\n",
        "def mount_drive_if_needed():\n",
        "    if not IN_COLAB:\n",
        "        return False\n",
        "    if Path(\"/content/drive/MyDrive\").exists():\n",
        "        print(\"[info] Drive already mounted\")\n",
        "        return True\n",
        "    try:\n",
        "        colab_drive.mount(\"/content/drive\", force_remount=False)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Drive mount failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def setup_drive_root(drive_ok):\n",
        "    global DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR, PUBLIC_URL_FILE\n",
        "    DRIVE_ROOT = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\") if drive_ok else Path(\"/content/MY-AI-Gizmo\")\n",
        "    LOG_DIR = DRIVE_ROOT / \"logs\"\n",
        "    MPL_CONFIG_DIR = DRIVE_ROOT / \"matplotlib\"\n",
        "    PUBLIC_URL_FILE = DRIVE_ROOT / \"public_url.txt\"\n",
        "    for p in (DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR, DRIVE_ROOT / \"models\", DRIVE_ROOT / \"settings\", DRIVE_ROOT / \"characters\"):\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# ---------------- Repo ----------------\n",
        "def check_repo_update():\n",
        "    if not WORK_DIR.exists():\n",
        "        return \"new\"\n",
        "    print(\"\\nDid you update your GitHub repo?\")\n",
        "    print(\"y = fresh re-clone, n = keep local\")\n",
        "    while True:\n",
        "        ans = input(\"(y/n): \").strip().lower()\n",
        "        if ans in (\"y\", \"yes\"): return \"fresh\"\n",
        "        if ans in (\"n\", \"no\"): return \"keep\"\n",
        "\n",
        "def apply_repo_update(mode):\n",
        "    if mode == \"fresh\" and WORK_DIR.exists():\n",
        "        _kill_old_servers()\n",
        "        shutil.rmtree(WORK_DIR, ignore_errors=True)\n",
        "\n",
        "def clone_repo():\n",
        "    print(\"[info] Cloning repository...\")\n",
        "    r = sh(f\"git clone --depth=1 {REPO_CLONE_URL} {WORK_DIR}\")\n",
        "    if r.returncode == 0 and WORK_DIR.exists():\n",
        "        return True\n",
        "\n",
        "    print(\"[warn] git clone failed, trying zip fallback...\")\n",
        "    tmp_zip = Path(\"/content/repo.zip\")\n",
        "    try: tmp_zip.unlink()\n",
        "    except Exception: pass\n",
        "\n",
        "    for cmd in (f\"wget -q -O {tmp_zip} '{REPO_ZIP}'\", f\"curl -s -L -o {tmp_zip} '{REPO_ZIP}'\"):\n",
        "        rr = sh(cmd)\n",
        "        if rr.returncode == 0 and tmp_zip.exists() and tmp_zip.stat().st_size > 1000:\n",
        "            break\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "    sh(f\"unzip -q {tmp_zip} -d /content\")\n",
        "    found = next(Path(\"/content\").glob(f\"{GITHUB_REPO}-*\"), None)\n",
        "    if not found:\n",
        "        return False\n",
        "    found.rename(WORK_DIR)\n",
        "    return True\n",
        "\n",
        "\n",
        "# ---------------- Models ----------------\n",
        "def list_local_models():\n",
        "    d = DRIVE_ROOT / \"models\"\n",
        "    if not d.exists():\n",
        "        return []\n",
        "    found = []\n",
        "    for ext in (\"*.gguf\", \"*.safetensors\", \"*.bin\"):\n",
        "        found.extend(d.rglob(ext))\n",
        "    return sorted(found)\n",
        "\n",
        "def choose_mode():\n",
        "    global USE_GPU, GPU_LAYERS, N_CTX\n",
        "    print(\"\\nChoose mode: [1] GPU  [2] CPU\")\n",
        "    while True:\n",
        "        c = input(\"1/2: \").strip()\n",
        "        if c == \"1\":\n",
        "            USE_GPU = True\n",
        "            GPU_LAYERS = -1\n",
        "            N_CTX = 4096\n",
        "            return\n",
        "        if c == \"2\":\n",
        "            USE_GPU = False\n",
        "            GPU_LAYERS = 0\n",
        "            N_CTX = 4096\n",
        "            return\n",
        "\n",
        "def choose_model():\n",
        "    global MODEL_REPO, MODEL_FILE, N_CTX, USE_MODEL\n",
        "    local = list_local_models()\n",
        "    print(\"\\nModel selector:\")\n",
        "    if local:\n",
        "        for i, m in enumerate(local, 1):\n",
        "            print(f\"  [L{i}] {m.name}\")\n",
        "    for row in MODEL_MENU:\n",
        "        print(\" \", row[0])\n",
        "    print(\"  [0] start without model\")\n",
        "\n",
        "    while True:\n",
        "        c = input(\"Choice: \").strip()\n",
        "        if c == \"0\":\n",
        "            USE_MODEL = False\n",
        "            MODEL_REPO = \"\"\n",
        "            MODEL_FILE = \"\"\n",
        "            return\n",
        "        if c.upper().startswith(\"L\") and local:\n",
        "            try:\n",
        "                idx = int(c[1:]) - 1\n",
        "                sel = local[idx]\n",
        "                USE_MODEL = True\n",
        "                MODEL_REPO = \"\"\n",
        "                MODEL_FILE = sel.name\n",
        "                N_CTX = auto_ctx_size(sel.stat().st_size / (1024 ** 3))\n",
        "                return\n",
        "            except Exception:\n",
        "                print(\"Invalid local selection.\")\n",
        "                continue\n",
        "        try:\n",
        "            idx = int(c) - 1\n",
        "            entry = MODEL_MENU[idx]\n",
        "            if entry[1]:\n",
        "                USE_MODEL = True\n",
        "                MODEL_REPO, MODEL_FILE = entry[1], entry[2]\n",
        "                N_CTX = auto_ctx_size(entry[3])\n",
        "                return\n",
        "            MODEL_REPO = input(\"HF repo: \").strip()\n",
        "            MODEL_FILE = input(\"Filename: \").strip()\n",
        "            USE_MODEL = True\n",
        "            N_CTX = 2048\n",
        "            return\n",
        "        except Exception:\n",
        "            print(\"Invalid choice.\")\n",
        "\n",
        "def download_model_if_missing():\n",
        "    if not USE_MODEL:\n",
        "        print(\"[info] Starting without model\")\n",
        "        return True\n",
        "    models_dir = DRIVE_ROOT / \"models\"\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "    model_path = models_dir / MODEL_FILE\n",
        "    if model_path.exists() and model_path.stat().st_size > 100 * 1024 * 1024:\n",
        "        print(\"[‚úì] Model already present\")\n",
        "        return True\n",
        "    if not MODEL_REPO:\n",
        "        return False\n",
        "\n",
        "    hf_url = f\"https://huggingface.co/{MODEL_REPO}/resolve/main/{MODEL_FILE}?download=true\"\n",
        "    for cmd in (\n",
        "        f'wget -q --show-progress -O \"{model_path}\" \"{hf_url}\"',\n",
        "        f'curl -L --progress-bar -o \"{model_path}\" \"{hf_url}\"',\n",
        "    ):\n",
        "        r = subprocess.run(cmd, shell=True)\n",
        "        if r.returncode == 0 and model_path.exists() and model_path.stat().st_size > 100 * 1024 * 1024:\n",
        "            print(\"[‚úì] Download complete\")\n",
        "            return True\n",
        "        try: model_path.unlink()\n",
        "        except Exception: pass\n",
        "    return False\n",
        "\n",
        "\n",
        "# ---------------- Config / links ----------------\n",
        "def ensure_symlinks_and_files():\n",
        "    links_map = [\n",
        "        (\"user_data/models\", \"models\", False),\n",
        "        (\"models\", \"models\", False),\n",
        "        (\"user_data/loras\", \"loras\", False),\n",
        "        (\"user_data/characters\", \"characters\", False),\n",
        "        (\"user_data/presets\", \"presets\", False),\n",
        "        (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "        (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "        (\"user_data/chat\", \"chat-history\", False),\n",
        "        (\"outputs\", \"outputs\", False),\n",
        "    ]\n",
        "    for local_rel, drive_rel, is_file in links_map:\n",
        "        drive_path = DRIVE_ROOT / drive_rel\n",
        "        local_path = WORK_DIR / local_rel\n",
        "        if is_file:\n",
        "            drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if not drive_path.exists():\n",
        "                drive_path.write_text(\"\", encoding=\"utf-8\")\n",
        "        else:\n",
        "            drive_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            if local_path.is_symlink() or local_path.is_file():\n",
        "                local_path.unlink()\n",
        "            elif local_path.is_dir():\n",
        "                shutil.rmtree(local_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        try:\n",
        "            os.symlink(str(drive_path), str(local_path), target_is_directory=not is_file)\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] symlink failed {local_rel}: {e}\")\n",
        "\n",
        "def write_settings_yaml():\n",
        "    threads = auto_thread_count()\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    model_line = f\"model: {MODEL_FILE}\" if (USE_MODEL and MODEL_FILE) else \"model: None\"\n",
        "    content = f\"\"\"# MY-AI-Gizmo Settings ‚Äî {mode_label}\n",
        "listen: true\n",
        "share: true\n",
        "auto_launch: false\n",
        "loader: llama.cpp\n",
        "n_ctx: {N_CTX}\n",
        "n_batch: 512\n",
        "n_gpu_layers: {GPU_LAYERS}\n",
        "threads: {threads}\n",
        "character: Debug\n",
        "{model_line}\n",
        "chat_style: cai-chat\n",
        "api: true\n",
        "api_port: 5000\n",
        "\"\"\"\n",
        "    for path in (WORK_DIR / \"user_data\" / \"settings.yaml\", DRIVE_ROOT / \"settings\" / \"settings.yaml\"):\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        path.write_text(content, encoding=\"utf-8\")\n",
        "\n",
        "def write_cmd_flags():\n",
        "    threads = auto_thread_count()\n",
        "    flags = [\n",
        "        \"--listen\", \"--share\", \"--verbose\",\n",
        "        \"--api\", \"--api-port\", \"5000\",\n",
        "        \"--loader\", \"llama.cpp\",\n",
        "        \"--gpu-layers\", str(GPU_LAYERS),\n",
        "        \"--ctx-size\", str(N_CTX),\n",
        "        \"--batch-size\", \"512\",\n",
        "        \"--threads\", str(threads),\n",
        "        \"--extensions\", EXTENSIONS,\n",
        "    ]\n",
        "    if USE_MODEL and MODEL_FILE:\n",
        "        flags += [\"--model\", MODEL_FILE]\n",
        "    content = \" \".join(flags)\n",
        "    for path in (WORK_DIR / \"user_data\" / \"CMD_FLAGS.txt\", DRIVE_ROOT / \"settings\" / \"CMD_FLAGS.txt\"):\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        path.write_text(content, encoding=\"utf-8\")\n",
        "\n",
        "def write_debug_character():\n",
        "    yaml = \"\"\"name: Debug\n",
        "greeting: \"DEBUG MODE ACTIVE ‚Äî fully verbose, technical. What do you need?\"\n",
        "context: |\n",
        "  You are in DEBUG MODE. Expert AI coding and general assistant.\n",
        "\"\"\"\n",
        "    for d in (WORK_DIR / \"user_data\" / \"characters\", DRIVE_ROOT / \"characters\"):\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "        (d / \"Debug.yaml\").write_text(yaml, encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "# ---------------- Optional extension stubs ----------------\n",
        "def _deploy_ext_stub(ext_name):\n",
        "    ext_dir = WORK_DIR / \"extensions\" / ext_name\n",
        "    ext_dir.mkdir(parents=True, exist_ok=True)\n",
        "    if (ext_dir / \"script.py\").exists():\n",
        "        return\n",
        "    stub = f'''\"\"\"Auto-stub for {ext_name}\"\"\"\n",
        "params = {{\"display_name\": \"{ext_name}\", \"is_tab\": True}}\n",
        "def ui():\n",
        "    import gradio as gr\n",
        "    gr.Markdown(\"## {ext_name}\\\\nUpload full extension from GitHub.\")\n",
        "'''\n",
        "    (ext_dir / \"script.py\").write_text(stub, encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "# ---------------- Wrapper ----------------\n",
        "def build_launch_wrapper(python_exe):\n",
        "    threads = auto_thread_count()\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    model_desc = MODEL_FILE if USE_MODEL else \"NO MODEL\"\n",
        "    cuda_block = \"os.environ['CUDA_VISIBLE_DEVICES'] = ''\" if not USE_GPU else \"\"\n",
        "    model_flag = f\"'--model', '{MODEL_FILE}',\" if (USE_MODEL and MODEL_FILE) else \"\"\n",
        "\n",
        "    code = f\"\"\"#!/usr/bin/env python3\n",
        "import sys, os\n",
        "{cuda_block}\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "os.environ['MPLCONFIGDIR'] = r'{MPL_CONFIG_DIR}'\n",
        "os.environ['GRADIO_SERVER_NAME'] = '0.0.0.0'\n",
        "os.environ['GRADIO_SHARE'] = '1'\n",
        "\n",
        "flags = [\n",
        "    '--listen', '--share', '--verbose',\n",
        "    '--api', '--api-port', '5000',\n",
        "    '--loader', 'llama.cpp',\n",
        "    '--gpu-layers', '{GPU_LAYERS}',\n",
        "    '--ctx-size', '{N_CTX}',\n",
        "    '--batch-size', '512',\n",
        "    '--threads', '{threads}',\n",
        "    {model_flag}\n",
        "    '--extensions', '{EXTENSIONS}',\n",
        "]\n",
        "flags = [f for f in flags if f]\n",
        "\n",
        "for f in flags:\n",
        "    if f not in sys.argv:\n",
        "        sys.argv.append(f)\n",
        "\n",
        "print('[WRAPPER v3.5.3] Mode: {mode_label} | Model: {model_desc}')\n",
        "print('[WRAPPER] Extensions: {EXTENSIONS}')\n",
        "\n",
        "# Gradio compatibility shim (4.37.x: no Timer)\n",
        "try:\n",
        "    import gradio as gr\n",
        "    if not hasattr(gr, 'Timer'):\n",
        "        class _GizmoTimerShim:\n",
        "            def __init__(self, *args, **kwargs): pass\n",
        "            def tick(self, *args, **kwargs): return None\n",
        "        gr.Timer = _GizmoTimerShim\n",
        "        print('[WRAPPER] Applied gr.Timer compatibility shim')\n",
        "except Exception as e:\n",
        "    print(f'[WRAPPER] Timer shim warning: {{e}}')\n",
        "\n",
        "try:\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg', force=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import traceback, runpy\n",
        "try:\n",
        "    runpy.run_path('server.py', run_name='__main__')\n",
        "except SystemExit:\n",
        "    pass\n",
        "except Exception:\n",
        "    print('\\\\n[ERROR] server.py raised an exception:')\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\"\"\"\n",
        "    wrapper_path = WORK_DIR / \"_gizmo_launch.py\"\n",
        "    wrapper_path.write_text(code, encoding=\"utf-8\")\n",
        "    return str(wrapper_path)\n",
        "\n",
        "\n",
        "# ---------------- Launch ----------------\n",
        "def launch(python_exe, wrapper_path):\n",
        "    cmd = [python_exe, \"-u\", wrapper_path]\n",
        "    env = os.environ.copy()\n",
        "    env.update({\n",
        "        \"MPLBACKEND\": \"Agg\",\n",
        "        \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "        \"GRADIO_SERVER_NAME\": \"0.0.0.0\",\n",
        "        \"GRADIO_SHARE\": \"1\",\n",
        "    })\n",
        "    if not USE_GPU:\n",
        "        env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "    captured = None\n",
        "\n",
        "    for attempt in range(1, MAX_RESTARTS + 1):\n",
        "        print(f\"\\n{'='*70}\\nüöÄ Starting server (attempt {attempt}/{MAX_RESTARTS})\\n{'='*70}\\n\")\n",
        "        if attempt > 1:\n",
        "            time.sleep(5)\n",
        "\n",
        "        log_path = LOG_DIR / f\"server_{int(time.time())}.log\"\n",
        "        logfile = open(log_path, \"a\", encoding=\"utf-8\")\n",
        "        os.chdir(WORK_DIR)\n",
        "\n",
        "        proc = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            env=env,\n",
        "            text=True,\n",
        "            bufsize=1\n",
        "        )\n",
        "\n",
        "        last_out = [time.time()]\n",
        "        stop_hb = threading.Event()\n",
        "\n",
        "        def heartbeat():\n",
        "            while not stop_hb.wait(HEARTBEAT_INTERVAL):\n",
        "                if time.time() - last_out[0] >= HEARTBEAT_INTERVAL:\n",
        "                    print(\"[heartbeat] server still running...\")\n",
        "\n",
        "        hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "        hb.start()\n",
        "\n",
        "        try:\n",
        "            for line in proc.stdout:\n",
        "                last_out[0] = time.time()\n",
        "                print(line, end=\"\", flush=True)\n",
        "                logfile.write(line)\n",
        "\n",
        "                if not captured:\n",
        "                    for pat in URL_PATTERNS:\n",
        "                        m = pat.search(line)\n",
        "                        if m:\n",
        "                            url = m.group(1).rstrip(\").,\\\\'\\\"\")\n",
        "                            if any(k in url.lower() for k in URL_KEYWORDS):\n",
        "                                captured = url\n",
        "                                print(f\"\\n{'='*70}\\nüåê PUBLIC URL: {captured}\\n{'='*70}\\n\")\n",
        "                                try:\n",
        "                                    PUBLIC_URL_FILE.write_text(captured)\n",
        "                                except Exception:\n",
        "                                    pass\n",
        "                                break\n",
        "        finally:\n",
        "            stop_hb.set()\n",
        "            hb.join(timeout=1)\n",
        "            logfile.close()\n",
        "\n",
        "        rc = proc.wait()\n",
        "        print(f\"\\n[info] Server exited with code {rc}\")\n",
        "\n",
        "        if rc in (0, -9):\n",
        "            break\n",
        "        if attempt < MAX_RESTARTS:\n",
        "            print(f\"[warn] Crashed (code {rc}) ‚Äî restarting...\")\n",
        "        else:\n",
        "            print(\"[info] Max restarts reached.\")\n",
        "\n",
        "    return captured\n",
        "\n",
        "\n",
        "# ---------------- Main ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    setup_github_token()\n",
        "\n",
        "    repo_mode = check_repo_update()\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"MY-AI-Gizmo v3.5.3 Universal Launcher\")\n",
        "    print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    choose_mode()\n",
        "    drive_ok = mount_drive_if_needed()\n",
        "    setup_drive_root(drive_ok)\n",
        "\n",
        "    apply_repo_update(repo_mode)\n",
        "    if not WORK_DIR.exists():\n",
        "        if not clone_repo():\n",
        "            raise SystemExit(\"‚ùå Repo clone failed. Check token/repo.\")\n",
        "\n",
        "    ensure_symlinks_and_files()\n",
        "    choose_model()\n",
        "    print_ram_status()\n",
        "    if not download_model_if_missing():\n",
        "        raise SystemExit(\"‚ùå Model download failed.\")\n",
        "\n",
        "    write_settings_yaml()\n",
        "    write_cmd_flags()\n",
        "    write_debug_character()\n",
        "\n",
        "    # Ensure expected extension dirs exist (does not overwrite real files)\n",
        "    _deploy_ext_stub(\"gizmo_toolbar\")\n",
        "    _deploy_ext_stub(\"google_workspace\")\n",
        "    _deploy_ext_stub(\"dual_model\")\n",
        "    _deploy_ext_stub(\"learning_center\")\n",
        "    _deploy_ext_stub(\"student_utils\")\n",
        "    _deploy_ext_stub(\"model_hub\")\n",
        "\n",
        "    start_sh = WORK_DIR / \"start_linux.sh\"\n",
        "    env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "    python_exe = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "\n",
        "    if not start_sh.exists():\n",
        "        raise SystemExit(\"‚ùå start_linux.sh not found.\")\n",
        "\n",
        "    sh(\"chmod +x start_linux.sh\", cwd=str(WORK_DIR))\n",
        "\n",
        "    if not env_marker.exists():\n",
        "        print(\"[info] First run: installing environment...\")\n",
        "        install_env = os.environ.copy()\n",
        "        install_env.update({\n",
        "            \"MPLBACKEND\": \"Agg\",\n",
        "            \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "            \"GPU_CHOICE\": \"A\" if USE_GPU else \"N\",\n",
        "            \"LAUNCH_AFTER_INSTALL\": \"FALSE\",\n",
        "            \"INSTALL_EXTENSIONS\": \"FALSE\",\n",
        "            \"SKIP_TORCH_TEST\": \"TRUE\",\n",
        "        })\n",
        "        if not USE_GPU:\n",
        "            install_env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "        subprocess.run(\"bash start_linux.sh\", shell=True, cwd=str(WORK_DIR), env=install_env)\n",
        "        python_exe = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "\n",
        "    _kill_old_servers()\n",
        "    wrapper_path = build_launch_wrapper(python_exe)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"LAUNCHING v3.5.3 ‚Äî {'GPU' if USE_GPU else 'CPU'}\")\n",
        "    print(f\"Model: {MODEL_FILE if USE_MODEL else '(none)'}\")\n",
        "    print(f\"Extensions: {EXTENSIONS}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    captured = launch(python_exe, wrapper_path)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    if captured:\n",
        "        print(f\"‚úÖ READY: {captured}\")\n",
        "        print(\"Tabs expected: Learning Center, Student Utils, Model Hub, Google Workspace, Dual Model\")\n",
        "    else:\n",
        "        print(\"‚ùå No public URL captured. Check logs in:\", LOG_DIR)\n",
        "    print(\"=\" * 70)\n"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "Qi3tWZ53XtPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# GIZMO AUTO MERGE SCRIPT (COLAB)\n",
        "# Accept ALL incoming changes\n",
        "# ===============================\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "print(\"\\n=== Gizmo Auto Merge Tool ===\\n\")\n",
        "\n",
        "# -------- USER INPUT --------\n",
        "\n",
        "repo_url = input(\"Enter GitHub repo URL (example: https://github.com/USER/REPO.git): \")\n",
        "\n",
        "branch = input(\"Target branch (usually main): \")\n",
        "if branch.strip() == \"\":\n",
        "    branch = \"main\"\n",
        "\n",
        "incoming_branch = input(\"Incoming branch to merge FROM: \")\n",
        "\n",
        "token = getpass.getpass(\"Paste GitHub Token (hidden): \")\n",
        "\n",
        "# -------- SETUP --------\n",
        "\n",
        "repo_name = repo_url.split(\"/\")[-1].replace(\".git\",\"\")\n",
        "\n",
        "auth_repo = repo_url.replace(\n",
        "    \"https://\",\n",
        "    f\"https://{token}@\"\n",
        ")\n",
        "\n",
        "print(\"\\nCloning or updating repo...\\n\")\n",
        "\n",
        "if os.path.exists(repo_name):\n",
        "    os.system(f\"rm -rf {repo_name}\")\n",
        "\n",
        "os.system(f\"git clone {auth_repo}\")\n",
        "\n",
        "os.chdir(repo_name)\n",
        "\n",
        "# -------- CONFIG --------\n",
        "\n",
        "os.system(\"git config user.email 'colab@gizmo.ai'\")\n",
        "os.system(\"git config user.name 'Colab Gizmo Bot'\")\n",
        "\n",
        "# -------- BACKUP --------\n",
        "\n",
        "print(\"\\nCreating backup branch...\\n\")\n",
        "\n",
        "os.system(f\"git checkout {branch}\")\n",
        "os.system(\"git checkout -b backup-before-merge\")\n",
        "\n",
        "# -------- MERGE --------\n",
        "\n",
        "print(\"\\nMerging incoming changes...\\n\")\n",
        "\n",
        "os.system(f\"git checkout {branch}\")\n",
        "os.system(\"git fetch origin\")\n",
        "\n",
        "merge_code = os.system(\n",
        "    f\"git merge -X theirs origin/{incoming_branch}\"\n",
        ")\n",
        "\n",
        "# -------- AUTO RESOLVE --------\n",
        "\n",
        "print(\"\\nResolving conflicts automatically...\\n\")\n",
        "\n",
        "os.system(\n",
        "    \"git diff --name-only --diff-filter=U | xargs -r git checkout --theirs --\"\n",
        ")\n",
        "\n",
        "os.system(\"git add -A\")\n",
        "\n",
        "os.system(\n",
        "    \"git commit -m 'Auto-resolve conflicts: accepted incoming changes'\"\n",
        ")\n",
        "\n",
        "# -------- PUSH --------\n",
        "\n",
        "print(\"\\nPushing to GitHub...\\n\")\n",
        "\n",
        "os.system(f\"git push origin {branch}\")\n",
        "\n",
        "# -------- DONE --------\n",
        "\n",
        "print(\"\\nSUCCESS!\")\n",
        "print(\"All incoming changes merged.\")\n",
        "print(\"Conflicts resolved automatically.\")\n",
        "print(\"Backup branch created: backup-before-merge\")"
      ],
      "metadata": {
        "id": "xz1cXI-NPY8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ RECOMMENDED MODELS (COPY EXACTLY)\n",
        "üîπ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "üîπ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "üîπ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "üîπ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "üîπ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "üîπ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "‚öôÔ∏è WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}