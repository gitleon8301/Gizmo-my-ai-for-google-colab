{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# MY-AI-Gizmo â€¢ LAUNCHER WITH VERBOSE INSTALLATION\n",
        "# Shows exactly what's happening during installation\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except:\n",
        "    drive = None\n",
        "\n",
        "# Configuration\n",
        "REPO_ZIP = \"https://github.com/gitleon8301/MY-AI-Gizmo-working/archive/refs/heads/main.zip\"\n",
        "WORK_DIR = Path(\"/content/text-generation-webui\")\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "\n",
        "def sh(cmd, check=False):\n",
        "    return subprocess.run(cmd, shell=True, capture_output=True, text=True, check=check)\n",
        "\n",
        "def sh_live(cmd):\n",
        "    \"\"\"Run command and show output in real-time\"\"\"\n",
        "    proc = subprocess.Popen(\n",
        "        cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "        text=True, bufsize=1\n",
        "    )\n",
        "    for line in proc.stdout:\n",
        "        print(line, end='')\n",
        "    proc.wait()\n",
        "    return proc.returncode\n",
        "\n",
        "print(\"ğŸ”§ Setting up environment...\")\n",
        "if \"MPLBACKEND\" in os.environ:\n",
        "    del os.environ[\"MPLBACKEND\"]\n",
        "os.environ[\"MPLBACKEND\"] = \"Agg\"\n",
        "print(\"âœ“ Environment ready\\n\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CUDA FIX\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def fix_cuda_library_path():\n",
        "    print(\"ğŸ”§ Fixing CUDA library paths...\")\n",
        "    cuda_paths = [\n",
        "        '/usr/local/cuda/lib64',\n",
        "        '/usr/local/cuda-12/lib64',\n",
        "        '/usr/lib/x86_64-linux-gnu',\n",
        "        '/usr/local/nvidia/lib64',\n",
        "    ]\n",
        "    valid_paths = []\n",
        "    for path in cuda_paths:\n",
        "        if Path(path).exists() and list(Path(path).glob('libcuda.so*')):\n",
        "            valid_paths.append(path)\n",
        "            print(f\"  âœ“ {path}\")\n",
        "    if valid_paths:\n",
        "        os.environ['LD_LIBRARY_PATH'] = ':'.join(valid_paths)\n",
        "        print(f\"  âœ“ Set LD_LIBRARY_PATH\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# FILE FUNCTIONS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def _ensure_drive_path(drive_path: Path, is_settings_file=False):\n",
        "    if drive_path.suffix:\n",
        "        drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        if is_settings_file and not drive_path.exists():\n",
        "            drive_path.touch(exist_ok=True)\n",
        "    else:\n",
        "        drive_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _remove_path(path: Path):\n",
        "    try:\n",
        "        if path.is_symlink():\n",
        "            path.unlink()\n",
        "        elif path.is_dir():\n",
        "            shutil.rmtree(path)\n",
        "        elif path.exists():\n",
        "            path.unlink()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "def _create_symlink_or_fallback(src: Path, dest: Path):\n",
        "    try:\n",
        "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "        if dest.exists() or dest.is_symlink():\n",
        "            _remove_path(dest)\n",
        "        os.symlink(str(src), str(dest), target_is_directory=src.is_dir())\n",
        "        return True\n",
        "    except:\n",
        "        try:\n",
        "            if src.is_dir():\n",
        "                if dest.exists():\n",
        "                    _remove_path(dest)\n",
        "                shutil.copytree(src, dest)\n",
        "            else:\n",
        "                dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "                shutil.copy2(src, dest)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "def cleanup_broken_files(drive_root: Path):\n",
        "    print(\"\\nğŸ§¹ Cleaning broken files...\")\n",
        "    models_dir = drive_root / \"models\"\n",
        "    if not models_dir.exists():\n",
        "        print(\"  No models directory yet\")\n",
        "        return\n",
        "    extensions = [\"*.gguf\", \"*.safetensors\", \"*.bin\", \"*.pth\", \"*.pt\"]\n",
        "    broken = []\n",
        "    for ext in extensions:\n",
        "        for f in models_dir.rglob(ext):\n",
        "            if f.stat().st_size < (100 * 1024):\n",
        "                broken.append(f)\n",
        "    if broken:\n",
        "        print(f\"  Found {len(broken)} broken files - deleting...\")\n",
        "        for f in broken:\n",
        "            try:\n",
        "                f.unlink()\n",
        "            except:\n",
        "                pass\n",
        "        print(f\"  âœ“ Cleaned\")\n",
        "    else:\n",
        "        print(\"  âœ“ No broken files\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# MAIN SETUP\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ğŸš€ MY-AI-Gizmo Setup (Verbose Mode)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1: Mount\n",
        "print(\"\\nğŸ“ Step 1/6: Mounting Drive...\")\n",
        "if drive:\n",
        "    try:\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"âœ“ Mounted\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  {e}\")\n",
        "\n",
        "# Cleanup\n",
        "cleanup_broken_files(DRIVE_ROOT)\n",
        "\n",
        "# Step 2: Create folders\n",
        "print(\"\\nğŸ’¾ Step 2/6: Creating folders...\")\n",
        "folders = [\"models\", \"loras\", \"training\", \"characters\", \"presets\", \"prompts\",\n",
        "           \"settings\", \"chat-history\", \"instruct-history\", \"outputs\", \"images\",\n",
        "           \"logs\", \"cache\", \"extensions\", \"softprompts\"]\n",
        "for f in folders:\n",
        "    (DRIVE_ROOT / f).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"âœ“ {len(folders)} folders\")\n",
        "\n",
        "# Step 3: Download repo\n",
        "print(\"\\nğŸ“¥ Step 3/6: Repository...\")\n",
        "if not WORK_DIR.exists():\n",
        "    sh(\"rm -f /content/repo.zip\")\n",
        "    print(\"  Downloading...\")\n",
        "    sh(f\"wget -q -O /content/repo.zip {REPO_ZIP}\")\n",
        "    sh(\"unzip -q /content/repo.zip -d /content\")\n",
        "    try:\n",
        "        next(Path(\"/content\").glob(\"MY-AI-Gizmo-working-*\")).rename(WORK_DIR)\n",
        "        print(\"âœ“ Downloaded\")\n",
        "    except:\n",
        "        print(\"âš ï¸  Failed\")\n",
        "else:\n",
        "    print(\"âœ“ Exists\")\n",
        "\n",
        "if WORK_DIR.exists():\n",
        "    os.chdir(WORK_DIR)\n",
        "\n",
        "# Step 4: Symlinks\n",
        "print(\"\\nğŸ”— Step 4/6: Linking...\")\n",
        "links_map = [\n",
        "    (\"models\", \"models\", False),\n",
        "    (\"loras\", \"loras\", False),\n",
        "    (\"user_data/characters\", \"characters\", False),\n",
        "    (\"user_data/presets\", \"presets\", False),\n",
        "    (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "    (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "    (\"user_data/chat\", \"chat-history\", False),\n",
        "    (\"outputs\", \"outputs\", False),\n",
        "]\n",
        "\n",
        "for local, drive_folder, is_settings in links_map:\n",
        "    drive_path = DRIVE_ROOT / drive_folder\n",
        "    _ensure_drive_path(drive_path, is_settings_file=is_settings)\n",
        "    local_path = WORK_DIR / local\n",
        "    if local_path.exists() or local_path.is_symlink():\n",
        "        _remove_path(local_path)\n",
        "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    _create_symlink_or_fallback(drive_path, local_path)\n",
        "print(\"âœ“ Linked\")\n",
        "\n",
        "# Step 5: Settings\n",
        "print(\"\\nâš™ï¸  Step 5/6: Settings...\")\n",
        "drive_settings = DRIVE_ROOT / \"settings\" / \"settings.yaml\"\n",
        "local_settings = WORK_DIR / \"user_data\" / \"settings.yaml\"\n",
        "local_settings.parent.mkdir(parents=True, exist_ok=True)\n",
        "if local_settings.is_symlink():\n",
        "    local_settings.unlink()\n",
        "if drive_settings.exists() and drive_settings.stat().st_size > 0:\n",
        "    shutil.copy2(drive_settings, local_settings)\n",
        "    print(\"âœ“ Copied from Drive\")\n",
        "else:\n",
        "    local_settings.write_text(\"# minimal\\nlisten: true\\nshare: true\\n\")\n",
        "    print(\"âœ“ Created\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Step 6: Install (WITH LIVE OUTPUT)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ“¦ Step 6/6: Installing dependencies...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"ğŸ“Š LIVE INSTALLATION OUTPUT (showing progress)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nThis may take 10-15 minutes on first run...\")\n",
        "print(\"Watch for these stages:\")\n",
        "print(\"  1ï¸âƒ£  Creating environment\")\n",
        "print(\"  2ï¸âƒ£  Installing PyTorch (~2GB)\")\n",
        "print(\"  3ï¸âƒ£  Installing packages\")\n",
        "print(\"  4ï¸âƒ£  Compiling llama-cpp (slowest part)\")\n",
        "print(\"  5ï¸âƒ£  Finishing up\")\n",
        "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "sh(\"chmod +x start_linux.sh\")\n",
        "\n",
        "# Run with LIVE output\n",
        "start_time = time.time()\n",
        "returncode = sh_live(\"GPU_CHOICE=A LAUNCH_AFTER_INSTALL=FALSE INSTALL_EXTENSIONS=FALSE bash start_linux.sh\")\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if returncode == 0:\n",
        "    print(f\"âœ… Installation complete ({elapsed:.1f}s)\")\n",
        "else:\n",
        "    print(f\"âš ï¸  Installation warnings (took {elapsed:.1f}s)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CUDA FIX\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ”§ Setting up CUDA...\")\n",
        "fix_cuda_library_path()\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# LAUNCH\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸŒ LAUNCHING UI\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "sh(\"pkill -9 -f python\")\n",
        "time.sleep(2)\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"MPLBACKEND\"] = \"Agg\"\n",
        "if \"PYTHONPATH\" in env:\n",
        "    del env[\"PYTHONPATH\"]\n",
        "\n",
        "python_exe = str(WORK_DIR / \"installer_files/env/bin/python\")\n",
        "if not Path(python_exe).exists():\n",
        "    python_exe = \"python3\"\n",
        "\n",
        "cmd = f\"{python_exe} -u server.py --share --listen\"\n",
        "\n",
        "proc = subprocess.Popen(\n",
        "    cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "    text=True, bufsize=1, env=env, cwd=str(WORK_DIR)\n",
        ")\n",
        "\n",
        "shown = False\n",
        "for line in proc.stdout:\n",
        "    print(line, end=\"\")\n",
        "\n",
        "    if not shown and (\"Running on public URL:\" in line or \"Running on local URL:\" in line):\n",
        "        if (m := re.search(r\"(https://[a-z0-9]+\\.gradio\\.live)\", line)):\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            print(\"âœ¨ SUCCESS! âœ¨\")\n",
        "            print(\"=\" * 70)\n",
        "            print(f\"\\nğŸŒ PUBLIC: {m.group(1)} â† CLICK!\")\n",
        "            print(\"\\nğŸ¯ Model tab â†’ Select model â†’ Load â†’ Chat!\")\n",
        "            print(\"=\" * 70 + \"\\n\")\n",
        "            shown = True\n",
        "\n",
        "print(\"\\nâœ“ Done\")"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… RECOMMENDED MODELS (COPY EXACTLY)\n",
        "ğŸ”¹ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "âš™ï¸ WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}